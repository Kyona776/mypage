robots.txtを活用することで、クロールされるコンテンツを制御して、有利なコンテンツをクロールさせることが可能になります。
robots.txtを活用するとクロールを最適化することができ、SEOに良い効果をもたらします。
テキストファイルで記述します。記述する内容は、

User-Agent
Disallow
Sitemap

User-Agentは、どのクローラーの動きを制御するかを指定します。たとえば、Googlebotと指定した場合、GoogleのWebクロールを制御することになります。しかし、基本的には「*」（すべてのクローラー、の意味）で指定して問題はありません。

Disallowは、クローラーのアクセスを制御するファイルを指定するものです。Disallowで指定されたファイルやディレクトリはクロールがブロックされます。Disallowで指定がない場合にはクロールが許可されることになるため、

Disallow:
といったように空白になっていれば、すべてのファイルやディレクトリがクロールの対象となります。

Disallow:/example/
と記載した場合には、このディレクトリ配下がブロックされます。

次にSitemapは、sitemap.xmlの場所をクローラーに伝えるものです。これを記述しておくとSitemapを積極的に読んでもらえるので、是非記載しておきましょう。

(例)
User-Agent:*
Disallow:/example/
Sitemap:http://aaaaaa.jp/sitemap